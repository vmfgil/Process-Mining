# -*- coding: utf-8 -*-
"""

AplicaÃ§Ã£o Web Streamlit para AnÃ¡lise de Processos de GestÃ£o de Recursos de TI (VersÃ£o Completa e Fiel).



Esta aplicaÃ§Ã£o Ã© uma traduÃ§Ã£o fiel de um notebook de anÃ¡lise de processos,

incorporando um dashboard completo com todas as 46 visualizaÃ§Ãµes originais, organizadas

de forma intuitiva com um sistema de navegaÃ§Ã£o melhorado para uma experiÃªncia de utilizador otimizada.

"""



# --- 1. IMPORTAÃ‡ÃƒO DE BIBLIOTECAS ---

import streamlit as st

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import networkx as nx

from io import StringIO, BytesIO

import warnings

from collections import Counter

import base64

import tempfile

import os



# Bibliotecas de Process Mining (PM4PY) e PDF

try:

Â  Â  import pm4py

Â  Â  from fpdf import FPDF

Â  Â  from pm4py.objects.log.util import dataframe_utils

Â  Â  from pm4py.objects.conversion.log import converter as log_converter

Â  Â  from pm4py.visualization.dfg import visualizer as dfg_visualizer

Â  Â  from pm4py.visualization.petri_net import visualizer as pn_visualizer

Â  Â  from pm4py.algo.discovery.inductive import algorithm as inductive_miner

Â  Â  from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner

Â  Â  from pm4py.algo.conformance.alignments.petri_net import algorithm as alignments

Â  Â  from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator

Â  Â  from pm4py.algo.evaluation.precision import algorithm as precision_evaluator

Â  Â  from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator

Â  Â  from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator

except ImportError:

Â  Â  st.error("Uma ou mais bibliotecas necessÃ¡rias (pm4py, fpdf) nÃ£o estÃ£o instaladas.")

Â  Â  st.stop()





# --- 2. CONFIGURAÃ‡ÃƒO DA PÃGINA E ESTADO DA SESSÃƒO ---

st.set_page_config(

Â  Â  page_title="Dashboard Completo de AnÃ¡lise de Processos",

Â  Â  page_icon="ğŸ“Š",

Â  Â  layout="wide",

Â  Â  initial_sidebar_state="expanded"

)



# InicializaÃ§Ã£o do estado da sessÃ£o

default_files = ['projects', 'tasks', 'resources', 'resource_allocations', 'dependencies']

if 'uploaded_files' not in st.session_state:

Â  Â  st.session_state.uploaded_files = {k: None for k in default_files}

if 'analysis_complete' not in st.session_state:

Â  Â  st.session_state.analysis_complete = False

if 'pre_mining_results' not in st.session_state:

Â  Â  st.session_state.pre_mining_results = {}

if 'post_mining_results' not in st.session_state:

Â  Â  st.session_state.post_mining_results = {}

if 'dataframes' not in st.session_state:

Â  Â  st.session_state.dataframes = {}



warnings.filterwarnings("ignore")



# --- 3. ESTÃ‰TICA E CSS PERSONALIZADO ---

st.markdown("""

<style>

Â  Â  /* Tema Principal */

Â  Â  .stApp {

Â  Â  Â  Â  background-color: #f0f2f6;

Â  Â  }

Â  Â  /* Estilo dos TÃ­tulos */

Â  Â  h1, h2, h3 {

Â  Â  Â  Â  color: #1E3A8A; /* Azul Escuro */

Â  Â  }

Â  Â  /* BotÃµes */

Â  Â  .stButton > button {

Â  Â  Â  Â  border-radius: 20px;

Â  Â  Â  Â  border: 1px solid #1E3A8A;

Â  Â  Â  Â  background-color: #3B82F6; /* Azul PrimÃ¡rio */

Â  Â  Â  Â  color: white;

Â  Â  }

Â  Â  .stButton > button:hover {

Â  Â  Â  Â  background-color: #1E3A8A;

Â  Â  Â  Â  color: white;

Â  Â  Â  Â  border: 1px solid #3B82F6;

Â  Â  }

Â  Â  /* Barra Lateral */

Â  Â  [data-testid="stSidebar"] {

Â  Â  Â  Â  background-color: #DBEAFE; /* Azul Claro */

Â  Â  }

</style>

""", unsafe_allow_html=True)





# --- 4. FUNÃ‡Ã•ES DE ANÃLISE (MODULARIZADAS) ---



@st.cache_data

def load_and_preprocess_data(uploaded_files):

Â  Â  try:

Â  Â  Â  Â  dfs = {name: pd.read_csv(StringIO(file.getvalue().decode('utf-8'))) for name, file in uploaded_files.items()}

Â  Â  Â  Â Â 

Â  Â  Â  Â  for name in default_files:

Â  Â  Â  Â  Â  Â  for col in ['project_id', 'task_id', 'resource_id', 'allocation_id']:

Â  Â  Â  Â  Â  Â  Â  Â  if col in dfs[name].columns:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dfs[name][col] = dfs[name][col].astype(str)



Â  Â  Â  Â  for df_name in ['projects', 'tasks']:

Â  Â  Â  Â  Â  Â  for col in ['start_date', 'end_date', 'planned_end_date']:

Â  Â  Â  Â  Â  Â  Â  Â  if col in dfs[df_name].columns:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dfs[df_name][col] = pd.to_datetime(dfs[df_name][col], errors='coerce')

Â  Â  Â  Â  dfs['resource_allocations']['allocation_date'] = pd.to_datetime(dfs['resource_allocations']['allocation_date'], errors='coerce')



Â  Â  Â  Â  df_projects = dfs['projects']

Â  Â  Â  Â  df_projects['days_diff'] = (df_projects['end_date'] - df_projects['planned_end_date']).dt.days

Â  Â  Â  Â  df_projects['actual_duration_days'] = (df_projects['end_date'] - df_projects['start_date']).dt.days

Â  Â  Â  Â  df_projects['completion_month'] = df_projects['end_date'].dt.to_period('M').astype(str)

Â  Â  Â  Â  df_projects['completion_quarter'] = df_projects['end_date'].dt.to_period('Q').astype(str)

Â  Â  Â  Â Â 

Â  Â  Â  Â  df_tasks = dfs['tasks']

Â  Â  Â  Â  df_tasks['task_duration_days'] = (df_tasks['end_date'] - df_tasks['start_date']).dt.days



Â  Â  Â  Â  df_alloc_costs = dfs['resource_allocations'].merge(dfs['resources'], on='resource_id')

Â  Â  Â  Â  df_alloc_costs['cost_of_work'] = df_alloc_costs['hours_worked'] * df_alloc_costs['cost_per_hour']

Â  Â  Â  Â Â 

Â  Â  Â  Â  project_aggregates = df_alloc_costs.groupby('project_id').agg(total_actual_cost=('cost_of_work', 'sum'), num_resources=('resource_id', 'nunique')).reset_index()



Â  Â  Â  Â  dep_counts = dfs['dependencies'].groupby('project_id').size().reset_index(name='dependency_count')

Â  Â  Â  Â  task_counts = dfs['tasks'].groupby('project_id').size().reset_index(name='task_count')

Â  Â  Â  Â  project_complexity = pd.merge(dep_counts, task_counts, on='project_id', how='outer').fillna(0)

Â  Â  Â  Â  project_complexity['complexity_ratio'] = (project_complexity['dependency_count'] / project_complexity['task_count']).fillna(0)

Â  Â  Â  Â Â 

Â  Â  Â  Â  df_projects = df_projects.merge(project_aggregates, on='project_id', how='left').merge(project_complexity, on='project_id', how='left')

Â  Â  Â  Â  df_projects['cost_diff'] = df_projects['total_actual_cost'] - df_projects['budget_impact']

Â  Â  Â  Â  df_projects['cost_per_day'] = df_projects['total_actual_cost'] / df_projects['actual_duration_days'].replace(0, np.nan)

Â  Â  Â  Â  dfs['projects'] = df_projects



Â  Â  Â  Â  allocations_to_merge = dfs['resource_allocations'].drop(columns=['project_id'], errors='ignore')

Â  Â  Â  Â  df_full_context = df_tasks.merge(df_projects, on='project_id', suffixes=('_task', '_project'))

Â  Â  Â  Â  df_full_context = df_full_context.merge(allocations_to_merge, on='task_id').merge(dfs['resources'], on='resource_id')

Â  Â  Â  Â  df_full_context['cost_of_work'] = df_full_context['hours_worked'] * df_full_context['cost_per_hour']

Â  Â  Â  Â  dfs['full_context'] = df_full_context



Â  Â  Â  Â  log_df = dfs['tasks'].merge(allocations_to_merge, on='task_id').merge(dfs['resources'], on='resource_id')

Â  Â  Â  Â  log_df.rename(columns={'project_id': 'case:concept:name', 'task_name': 'concept:name', 'end_date': 'time:timestamp', 'resource_name': 'org:resource'}, inplace=True)

Â  Â  Â  Â  log_df['case:concept:name'] = 'Projeto ' + log_df['case:concept:name']

Â  Â  Â  Â  log_df.dropna(subset=['time:timestamp'], inplace=True)

Â  Â  Â  Â  log_df = log_df.sort_values('time:timestamp')

Â  Â  Â  Â  dfs['log_df'] = log_df

Â  Â  Â  Â  dfs['event_log'] = log_converter.apply(log_df)



Â  Â  Â  Â  return dfs

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro no prÃ©-processamento: {e}")

Â  Â  Â  Â  return None



def generate_pre_mining_visuals(dfs):

Â  Â  results = {}

Â  Â  df_projects, df_full_context, df_tasks, df_resources, log_df = dfs['projects'], dfs['full_context'], dfs['tasks'], dfs['resources'], dfs['log_df']

Â  Â Â 

Â  Â  results['kpis'] = {'Total de Projetos': df_projects['project_id'].nunique(), 'Total de Tarefas': df_tasks['task_id'].nunique(), 'Total de Eventos': len(log_df), 'Total de Recursos': df_resources['resource_id'].nunique(), 'DuraÃ§Ã£o MÃ©dia (dias)': f"{df_projects['actual_duration_days'].mean():.2f}"}

Â  Â Â 

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.scatterplot(data=df_projects, x='days_diff', y='cost_diff', hue='path_name', s=80, alpha=0.7, ax=ax); ax.axhline(0, c='k', ls='--'); ax.axvline(0, c='k', ls='--'); ax.set_title('Matriz de Performance: Prazo vs. OrÃ§amento'); results['plot_01'] = fig

Â  Â  fig, ax = plt.subplots(figsize=(8, 3)); sns.boxplot(x=df_projects['actual_duration_days'], color='skyblue', ax=ax); ax.set_title('DistribuiÃ§Ã£o da DuraÃ§Ã£o dos Projetos'); results['plot_02'] = fig

Â  Â Â 

Â  Â  lead_times = log_df.groupby("case:concept:name")["time:timestamp"].agg(["min", "max"]).reset_index()

Â  Â  lead_times["lead_time_days"] = (lead_times["max"] - lead_times["min"]).dt.days

Â  Â  fig, ax = plt.subplots(figsize=(8, 3)); sns.histplot(lead_times["lead_time_days"], bins=20, kde=True, ax=ax); ax.set_title('DistribuiÃ§Ã£o do Lead Time por Caso (dias)'); results['plot_03'] = fig

Â  Â Â 

Â  Â  throughput_per_case = log_df.groupby("case:concept:name").apply(lambda g: g['time:timestamp'].diff().mean().total_seconds() / 3600).reset_index(name="avg_throughput_hours")

Â  Â  fig, axes = plt.subplots(1, 2, figsize=(10, 3)); sns.histplot(throughput_per_case["avg_throughput_hours"], bins=20, kde=True, ax=axes[0], color='green'); axes[0].set_title('DistribuiÃ§Ã£o do Throughput (horas)'); sns.boxplot(x=throughput_per_case["avg_throughput_hours"], ax=axes[1], color='lightgreen'); axes[1].set_title('Boxplot do Throughput'); fig.tight_layout(); results['plot_04_05'] = fig

Â  Â Â 

Â  Â  perf_df = pd.merge(lead_times, throughput_per_case, on="case:concept:name")

Â  Â  fig, ax = plt.subplots(figsize=(7, 4)); sns.regplot(x="avg_throughput_hours", y="lead_time_days", data=perf_df, ax=ax); ax.set_title('RelaÃ§Ã£o entre Lead Time e Throughput'); results['plot_06'] = fig

Â  Â Â 

Â  Â  service_times = df_full_context.groupby('task_name')['hours_worked'].mean().reset_index()

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x='hours_worked', y='task_name', data=service_times.sort_values('hours_worked', ascending=False).head(10), palette='viridis', ax=ax, hue='task_name', legend=False); ax.set_title('Tempo MÃ©dio de ExecuÃ§Ã£o por Atividade (Horas)'); results['plot_07'] = fig

Â  Â Â 

Â  Â  df_handoff = log_df[log_df.duplicated(subset=['case:concept:name'], keep=False)].sort_values(['case:concept:name', 'time:timestamp'])

Â  Â  df_handoff['previous_activity_end_time'] = df_handoff.groupby('case:concept:name')['time:timestamp'].shift(1)

Â  Â  df_handoff['handoff_time_days'] = (df_handoff['time:timestamp'] - df_handoff['previous_activity_end_time']).dt.total_seconds() / (24*3600)

Â  Â  df_handoff['previous_activity'] = df_handoff.groupby('case:concept:name')['concept:name'].shift(1)

Â  Â  handoff_stats = df_handoff.groupby(['previous_activity', 'concept:name'])['handoff_time_days'].mean().reset_index().sort_values('handoff_time_days', ascending=False)

Â  Â  handoff_stats['transition'] = handoff_stats['previous_activity'].fillna('') + ' -> ' + handoff_stats['concept:name'].fillna('')

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(data=handoff_stats.head(10), y='transition', x='handoff_time_days', palette='magma', ax=ax, hue='transition', legend=False); ax.set_title('Top 10 TransiÃ§Ãµes com Maior Tempo de Espera'); results['plot_08'] = fig

Â  Â Â 

Â  Â  handoff_stats['estimated_cost_of_wait'] = handoff_stats['handoff_time_days'] * df_projects['cost_per_day'].mean()

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(data=handoff_stats.sort_values('estimated_cost_of_wait', ascending=False).head(10), y='transition', x='estimated_cost_of_wait', palette='Reds_r', ax=ax, hue='transition', legend=False); ax.set_title('Top 10 TransiÃ§Ãµes por Custo de Espera Estimado (â‚¬)'); results['plot_09'] = fig

Â  Â Â 

Â  Â  activity_counts = df_tasks["task_name"].value_counts()

Â  Â  fig, ax = plt.subplots(figsize=(8, 4)); sns.barplot(x=activity_counts.head(10).values, y=activity_counts.head(10).index, ax=ax, palette='plasma', hue=activity_counts.head(10).index, legend=False); ax.set_title('Atividades Mais Frequentes'); results['plot_10'] = fig

Â  Â Â 

Â  Â  resource_workload = df_full_context.groupby('resource_name')['hours_worked'].sum().sort_values(ascending=False)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x=resource_workload.head(10).values, y=resource_workload.head(10).index, ax=ax, palette='magma', hue=resource_workload.head(10).index, legend=False); ax.set_title('Top 10 Recursos por Horas Trabalhadas'); results['plot_11'] = fig

Â  Â Â 

Â  Â  resource_metrics = df_full_context.groupby("resource_name").agg(unique_cases=('project_id', 'nunique'), event_count=('task_id', 'count')).reset_index()

Â  Â  resource_metrics["avg_events_per_case"] = resource_metrics["event_count"] / resource_metrics["unique_cases"]

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x='avg_events_per_case', y='resource_name', data=resource_metrics.sort_values('avg_events_per_case', ascending=False).head(10), palette='coolwarm', ax=ax, hue='resource_name', legend=False); ax.set_title('Top 10 Recursos por MÃ©dia de Tarefas por Projeto'); results['plot_12'] = fig

Â  Â Â 

Â  Â  resource_activity_matrix_pivot = df_full_context.pivot_table(index='resource_name', columns='task_name', values='hours_worked', aggfunc='sum').fillna(0)

Â  Â  fig, ax = plt.subplots(figsize=(12, 8)); sns.heatmap(resource_activity_matrix_pivot, cmap='YlGnBu', annot=True, fmt=".0f", ax=ax); ax.set_title('Heatmap de EsforÃ§o (Horas) por Recurso e Atividade'); results['plot_13'] = fig

Â  Â Â 

Â  Â  handoff_counts = Counter()

Â  Â  for _, trace in log_df.groupby('case:concept:name'):

Â  Â  Â  Â  resources = trace['org:resource'].tolist()

Â  Â  Â  Â  for i in range(len(resources) - 1):

Â  Â  Â  Â  Â  Â  if resources[i] != resources[i+1]: handoff_counts[(resources[i], resources[i+1])] += 1

Â  Â  df_resource_handoffs = pd.DataFrame([{'De': k[0], 'Para': k[1], 'Contagem': v} for k,v in handoff_counts.items()]).sort_values('Contagem', ascending=False)

Â  Â  df_resource_handoffs['Handoff'] = df_resource_handoffs['De'] + ' -> ' + df_resource_handoffs['Para']

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x='Contagem', y='Handoff', data=df_resource_handoffs.head(10), palette='rocket', ax=ax, hue='Handoff', legend=False); ax.set_title('Top 10 Handoffs entre Recursos'); results['plot_14'] = fig

Â  Â Â 

Â  Â  cost_by_resource_type = df_full_context.groupby('resource_type')['cost_of_work'].sum().sort_values(ascending=False)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x=cost_by_resource_type.values, y=cost_by_resource_type.index, ax=ax, palette='cividis', hue=cost_by_resource_type.index, legend=False); ax.set_title('Custo Total por Tipo de Recurso'); results['plot_15'] = fig

Â  Â Â 

Â  Â  variants_df = log_df.groupby('case:concept:name')['concept:name'].apply(lambda x: ' -> '.join(x)).reset_index(name='variant_str')

Â  Â  variant_analysis = variants_df['variant_str'].value_counts().reset_index(name='frequency')

Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); sns.barplot(x='frequency', y='variant_str', data=variant_analysis.head(10), palette='coolwarm', ax=ax, hue='variant_str', legend=False); ax.set_title('Top 10 Variantes de Processo por FrequÃªncia'); results['plot_16'] = fig

Â  Â Â 

Â  Â  min_res, max_res = df_projects['num_resources'].min(), df_projects['num_resources'].max()

Â  Â  bins = np.linspace(min_res, max_res, 4, dtype=int) if max_res > min_res else [min_res, max_res]

Â  Â  df_projects['team_size_bin'] = pd.cut(df_projects['num_resources'], bins=bins, include_lowest=True, duplicates='drop').astype(str)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.boxplot(data=df_projects, x='team_size_bin', y='days_diff', ax=ax, palette='flare', hue='team_size_bin', legend=False); ax.set_title('Impacto do Tamanho da Equipa no Atraso'); results['plot_17'] = fig

Â  Â Â 

Â  Â  median_duration_by_team_size = df_projects.groupby('team_size_bin')['actual_duration_days'].median().reset_index()

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(data=median_duration_by_team_size, x='team_size_bin', y='actual_duration_days', palette='crest', ax=ax, hue='team_size_bin', legend=False); ax.set_title('DuraÃ§Ã£o Mediana por Tamanho da Equipa'); results['plot_18'] = fig

Â  Â Â 

Â  Â  df_full_context['day_of_week'] = df_full_context['allocation_date'].dt.day_name()

Â  Â  weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

Â  Â  weekly_hours = df_full_context.groupby('day_of_week')['hours_worked'].sum().reindex(weekday_order)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.barplot(x=weekly_hours.index, y=weekly_hours.values, ax=ax, palette='plasma', hue=weekly_hours.index, legend=False); ax.set_title('Horas Trabalhadas por Dia da Semana'); results['plot_19'] = fig

Â  Â Â 

Â  Â  df_tasks_analysis = df_tasks.copy()

Â  Â  df_tasks_analysis['service_time_days'] = df_tasks_analysis['task_duration_days']

Â  Â  df_tasks_analysis = df_tasks_analysis.sort_values(['project_id', 'start_date'])

Â  Â  df_tasks_analysis['previous_task_end'] = df_tasks_analysis.groupby('project_id')['end_date'].shift(1)

Â  Â  df_tasks_analysis['waiting_time_days'] = (df_tasks_analysis['start_date'] - df_tasks_analysis['previous_task_end']).dt.total_seconds() / (24*3600)

Â  Â  df_tasks_analysis['waiting_time_days'] = df_tasks_analysis['waiting_time_days'].clip(lower=0)

Â  Â Â 

Â  Â  df_tasks_with_resources = df_tasks_analysis.merge(df_full_context[['task_id', 'resource_name']], on='task_id', how='left').drop_duplicates()

Â  Â  bottleneck_by_resource = df_tasks_with_resources.groupby('resource_name')['waiting_time_days'].mean().sort_values(ascending=False).head(15)

Â  Â  fig, ax = plt.subplots(figsize=(8, 6)); sns.barplot(y=bottleneck_by_resource.index, x=bottleneck_by_resource.values, palette='rocket', ax=ax, hue=bottleneck_by_resource.index, legend=False); ax.set_title('Recursos por Tempo MÃ©dio de Espera (Dias)'); results['plot_20'] = fig

Â  Â Â 

Â  Â  bottleneck_by_activity = df_tasks_analysis.groupby('task_type')[['service_time_days', 'waiting_time_days']].mean()

Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); bottleneck_by_activity.plot(kind='bar', stacked=True, color=['royalblue', 'crimson'], ax=ax); ax.set_title('Gargalos (Tempo de ServiÃ§o vs. Espera)'); results['plot_21'] = fig

Â  Â  fig, ax = plt.subplots(figsize=(7, 4)); sns.regplot(data=bottleneck_by_activity, x='service_time_days', y='waiting_time_days', ax=ax); ax.set_title('Espera vs. ExecuÃ§Ã£o'); results['plot_22'] = fig



Â  Â  df_wait_over_time = df_tasks_analysis.merge(df_projects[['project_id', 'completion_month']], on='project_id')

Â  Â  monthly_wait_time = df_wait_over_time.groupby('completion_month')['waiting_time_days'].mean().reset_index()

Â  Â  fig, ax = plt.subplots(figsize=(10, 5)); sns.lineplot(data=monthly_wait_time, x='completion_month', y='waiting_time_days', marker='o', ax=ax); ax.set_title("EvoluÃ§Ã£o do Tempo MÃ©dio de Espera"); plt.xticks(rotation=45); results['plot_23'] = fig

Â  Â Â 

Â  Â  df_rh_typed = df_resource_handoffs.merge(df_resources[['resource_name', 'resource_type']], left_on='De', right_on='resource_name').merge(df_resources[['resource_name', 'resource_type']], left_on='Para', right_on='resource_name', suffixes=('_de', '_para'))

Â  Â  handoff_matrix = df_rh_typed.groupby(['resource_type_de', 'resource_type_para'])['Contagem'].sum().unstack().fillna(0)

Â  Â  fig, ax = plt.subplots(figsize=(8, 6)); sns.heatmap(handoff_matrix, annot=True, fmt=".0f", cmap="BuPu", ax=ax); ax.set_title("Matriz de Handoffs por Tipo de Equipa"); results['plot_24'] = fig

Â  Â Â 

Â  Â  perf_df['project_id'] = perf_df['case:concept:name'].str.replace('Projeto ', '')

Â  Â  df_perf_full = perf_df.merge(df_projects, on='project_id', how='left')

Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); sns.boxplot(data=df_perf_full, x='team_size_bin', y='avg_throughput_hours', palette='plasma', ax=ax, hue='team_size_bin', legend=False); ax.set_title('Benchmark de Throughput por Tamanho da Equipa'); results['plot_25'] = fig

Â  Â Â 

Â  Â  def get_phase(task_type):

Â  Â  Â  Â  if task_type in ['Desenvolvimento', 'CorreÃ§Ã£o', 'RevisÃ£o', 'Design']: return 'Desenvolvimento & Design'

Â  Â  Â  Â  if task_type == 'Teste': return 'Teste (QA)'

Â  Â  Â  Â  if task_type in ['Deploy', 'DBA']: return 'OperaÃ§Ãµes & Deploy'

Â  Â  Â  Â  return 'Outros'

Â  Â  df_tasks_phases = df_tasks.copy(); df_tasks_phases['phase'] = df_tasks_phases['task_type'].apply(get_phase)

Â  Â  phase_times = df_tasks_phases.groupby(['project_id', 'phase']).agg(start=('start_date', 'min'), end=('end_date', 'max')).reset_index()

Â  Â  phase_times['cycle_time_days'] = (phase_times['end'] - phase_times['start']).dt.days

Â  Â  avg_cycle_time_by_phase = phase_times.groupby('phase')['cycle_time_days'].mean()

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); avg_cycle_time_by_phase.plot(kind='bar', color=sns.color_palette('muted'), ax=ax); ax.set_title('DuraÃ§Ã£o MÃ©dia por Fase do Processo'); results['plot_26'] = fig

Â  Â Â 

Â  Â  return results



def calculate_model_metrics(log, petri_net, initial_marking, final_marking, title):

Â  Â  fitness = replay_fitness_evaluator.apply(log, petri_net, initial_marking, final_marking, variant=replay_fitness_evaluator.Variants.TOKEN_BASED)

Â  Â  precision = precision_evaluator.apply(log, petri_net, initial_marking, final_marking, variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)

Â  Â  generalization = generalization_evaluator.apply(log, petri_net, initial_marking, final_marking)

Â  Â  simplicity = simplicity_evaluator.apply(petri_net)

Â  Â  metrics = {"Fitness": fitness.get('average_trace_fitness', 0), "PrecisÃ£o": precision, "GeneralizaÃ§Ã£o": generalization, "Simplicidade": simplicity}

Â  Â Â 

Â  Â  df_metrics = pd.DataFrame(list(metrics.items()), columns=['MÃ©trica', 'Valor'])

Â  Â  fig, ax = plt.subplots(figsize=(8, 4))

Â  Â  sns.barplot(data=df_metrics, x='MÃ©trica', y='Valor', palette='viridis', ax=ax, hue='MÃ©trica', legend=False)

Â  Â  ax.set_ylim(0, 1.05); ax.set_ylabel(''); ax.set_xlabel(''); ax.set_title(title)

Â  Â  for p in ax.patches:

Â  Â  Â  Â  ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 9), textcoords='offset points')

Â  Â  return fig



def generate_post_mining_visuals(dfs):

Â  Â  results = {}

Â  Â  event_log, df_tasks, df_projects, log_df = dfs['event_log'], dfs['tasks'], dfs['projects'], dfs['log_df']

Â  Â Â 

Â  Â  process_tree_im = inductive_miner.apply(event_log)

Â  Â  net_im, im_im, fm_im = pm4py.convert_to_petri_net(process_tree_im)

Â  Â  results['model_01_inductive'] = pn_visualizer.apply(net_im, im_im, fm_im)

Â  Â  results['metrics_inductive'] = calculate_model_metrics(event_log, net_im, im_im, fm_im, 'MÃ©tricas de Qualidade (Inductive)')



Â  Â  net_hm, im_hm, fm_hm = heuristics_miner.apply(event_log)

Â  Â  results['model_02_heuristics'] = pn_visualizer.apply(net_hm, im_hm, fm_hm)

Â  Â  results['metrics_heuristics'] = calculate_model_metrics(event_log, net_hm, im_hm, fm_hm, 'MÃ©tricas de Qualidade (Heuristics)')

Â  Â Â 

Â  Â  dfg_perf, _, _ = pm4py.discover_performance_dfg(event_log)

Â  Â  results['model_03_performance_dfg'] = dfg_visualizer.apply(dfg_perf, log=event_log, variant=dfg_visualizer.Variants.PERFORMANCE)



Â  Â  variants = pm4py.get_variants_as_tuples(event_log)

Â  Â  variants_counts = {str(k): len(v) for k, v in variants.items()}

Â  Â  variants_df_full = pd.DataFrame(list(variants_counts.items()), columns=['variant', 'count']).sort_values(by='count', ascending=False)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); ax.pie(variants_df_full['count'].head(7), labels=[f'Variante {i+1}' for i in range(7)], autopct='%1.1f%%', startangle=90); ax.set_title('DistribuiÃ§Ã£o das 7 Variantes Mais Comuns'); results['chart_04_variants_pie'] = fig

Â  Â Â 

Â  Â  aligned_traces = alignments.apply(event_log, net_im, im_im, fm_im)

Â  Â  fitness_values = [trace['fitness'] for trace in aligned_traces]

Â  Â  fig, ax = plt.subplots(figsize=(8, 4)); sns.histplot(fitness_values, bins=20, kde=True, ax=ax, color='green'); ax.set_title('DistribuiÃ§Ã£o do Fitness de Conformidade'); results['chart_05_conformance_fitness'] = fig

Â  Â Â 

Â  Â  kpi_temporal = df_projects.groupby('completion_month').agg(avg_lead_time=('actual_duration_days', 'mean'), throughput=('project_id', 'count')).reset_index()

Â  Â  fig, ax1 = plt.subplots(figsize=(10, 5)); ax1.plot(kpi_temporal['completion_month'], kpi_temporal['avg_lead_time'], marker='o', color='b'); ax1.set_ylabel('Dias', color='b'); ax2 = ax1.twinx(); ax2.bar(kpi_temporal['completion_month'], kpi_temporal['throughput'], color='g', alpha=0.6); ax2.set_ylabel('NÂº de Projetos', color='g'); fig.suptitle('SÃ©ries Temporais de KPIs de Performance'); results['chart_06_kpi_time_series'] = fig

Â  Â Â 

Â  Â  fig, ax = plt.subplots(figsize=(12, 8)); projects_to_plot = df_projects.sort_values('start_date').head(20); tasks_to_plot = df_tasks[df_tasks['project_id'].isin(projects_to_plot['project_id'])]; project_y_map = {proj_id: i for i, proj_id in enumerate(projects_to_plot['project_id'])}; task_colors = plt.get_cmap('viridis', tasks_to_plot['task_name'].nunique()); color_map = {name: task_colors(i) for i, name in enumerate(tasks_to_plot['task_name'].unique())}; [ax.barh(project_y_map[task['project_id']], (task['end_date'] - task['start_date']).days + 1, left=task['start_date'], color=color_map.get(task['task_name'])) for _, task in tasks_to_plot.iterrows() if task['project_id'] in project_y_map]; ax.set_yticks(list(project_y_map.values())); ax.set_yticklabels([f"Projeto {pid}" for pid in project_y_map.keys()]); ax.invert_yaxis(); ax.set_title('GrÃ¡fico de Gantt (20 Primeiros Projetos)'); results['chart_07_gantt_chart'] = fig

Â  Â Â 

Â  Â  variants_df_log = log_df.groupby('case:concept:name').agg(variant=('concept:name', tuple), start=('time:timestamp', 'min'), end=('time:timestamp', 'max')).reset_index()

Â  Â  variants_df_log['duration_hours'] = (variants_df_log['end'] - variants_df_log['start']).dt.total_seconds() / 3600

Â  Â  variant_durations = variants_df_log.groupby('variant')['duration_hours'].mean().reset_index().sort_values('duration_hours', ascending=False)

Â  Â  variant_durations['variant_str'] = variant_durations['variant'].astype(str)

Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); sns.barplot(x='duration_hours', y='variant_str', data=variant_durations.head(10), palette='plasma', ax=ax, hue='variant_str', legend=False); ax.set_title('DuraÃ§Ã£o MÃ©dia das 10 Variantes Mais Lentas'); results['chart_08_variant_duration'] = fig

Â  Â Â 

Â  Â  deviations_list = [{'fitness': trace['fitness'], 'deviations': sum(1 for move in trace['alignment'] if '>>' in move[0] or '>>' in move[1])} for trace in aligned_traces]

Â  Â  deviations_df = pd.DataFrame(deviations_list)

Â  Â  fig, ax = plt.subplots(figsize=(8, 5)); sns.scatterplot(x='fitness', y='deviations', data=deviations_df, alpha=0.6, ax=ax); ax.set_title('Diagrama de DispersÃ£o (Fitness vs. Desvios)'); results['chart_09_deviation_scatter'] = fig



Â  Â  case_fitness_df = pd.DataFrame([{'project_id': trace.attributes['concept:name'].replace('Projeto ', ''), 'fitness': alignment['fitness']} for trace, alignment in zip(event_log, aligned_traces)])

Â  Â  case_fitness_df = case_fitness_df.merge(df_projects[['project_id', 'end_date']], on='project_id')

Â  Â  case_fitness_df['end_month'] = case_fitness_df['end_date'].dt.to_period('M').astype(str)

Â  Â  monthly_fitness = case_fitness_df.groupby('end_month')['fitness'].mean().reset_index()

Â  Â  fig, ax = plt.subplots(figsize=(10, 5)); sns.lineplot(data=monthly_fitness, x='end_month', y='fitness', marker='o', ax=ax); ax.set_title('Score de Conformidade ao Longo do Tempo'); plt.xticks(rotation=45); results['chart_10_conformance_over_time'] = fig

Â  Â Â 

Â  Â  df_projects_sorted = df_projects.sort_values(by='end_date')

Â  Â  df_projects_sorted['cumulative_throughput'] = range(1, len(df_projects_sorted) + 1)

Â  Â  fig, ax = plt.subplots(figsize=(10, 5)); sns.lineplot(x='end_date', y='cumulative_throughput', data=df_projects_sorted, ax=ax); ax.set_title('GrÃ¡fico Acumulado de Throughput'); results['chart_11_cumulative_throughput'] = fig

Â  Â Â 

Â  Â  milestones = ['Analise e Design', 'Implementacao da Funcionalidade', 'Execucao de Testes', 'Deploy da Aplicacao']

Â  Â  df_milestones = df_tasks[df_tasks['task_name'].isin(milestones)].sort_values(['project_id', 'start_date'])

Â  Â  milestone_pairs = []

Â  Â  for _, group in df_milestones.groupby('project_id'):

Â  Â  Â  Â  for i in range(len(group) - 1):

Â  Â  Â  Â  Â  Â  start_task, end_task = group.iloc[i], group.iloc[i+1]

Â  Â  Â  Â  Â  Â  duration = (end_task['start_date'] - start_task['end_date']).total_seconds() / 3600

Â  Â  Â  Â  Â  Â  if duration >= 0: milestone_pairs.append({'transition': f"{start_task['task_name']} -> {end_task['task_name']}", 'duration_hours': duration})

Â  Â  milestone_df = pd.DataFrame(milestone_pairs)

Â  Â  if not milestone_df.empty:

Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); sns.boxplot(data=milestone_df, x='duration_hours', y='transition', ax=ax, orient='h', palette='viridis'); ax.set_title('AnÃ¡lise de Tempo entre Marcos do Processo'); results['chart_12_milestone_analysis'] = fig

Â  Â  else:

Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(8,4)); ax.text(0.5, 0.5, 'Dados insuficientes para anÃ¡lise de marcos.', ha='center'); results['chart_12_milestone_analysis'] = fig

Â  Â  Â  Â Â 

Â  Â  df_tasks_sorted = df_tasks.sort_values(['project_id', 'start_date'])

Â  Â  df_tasks_sorted['previous_end_date'] = df_tasks_sorted.groupby('project_id')['end_date'].shift(1)

Â  Â  df_tasks_sorted['waiting_time_days'] = (df_tasks_sorted['start_date'] - df_tasks_sorted['previous_end_date']).dt.total_seconds() / (24 * 3600)

Â  Â  df_tasks_sorted.loc[df_tasks_sorted['waiting_time_days'] < 0, 'waiting_time_days'] = 0

Â  Â  df_tasks_sorted['previous_task_name'] = df_tasks_sorted.groupby('project_id')['task_name'].shift(1)

Â  Â  waiting_times_matrix = df_tasks_sorted.pivot_table(index='previous_task_name', columns='task_name', values='waiting_time_days', aggfunc='mean').fillna(0)

Â  Â  fig, ax = plt.subplots(figsize=(12, 10)); sns.heatmap(waiting_times_matrix, cmap='YlGnBu', annot=True, fmt='.2f', linewidths=.5, ax=ax); ax.set_title('Matriz de Tempo de Espera entre Atividades (dias)'); results['chart_13_waiting_time_matrix'] = fig

Â  Â Â 

Â  Â  waiting_time_by_task = df_tasks_sorted.groupby('task_name')['waiting_time_days'].mean().sort_values(ascending=False)

Â  Â  fig, ax = plt.subplots(figsize=(10, 6)); sns.barplot(x=waiting_time_by_task.values, y=waiting_time_by_task.index, ax=ax, palette='viridis', hue=waiting_time_by_task.index, legend=False); ax.set_title('Tempo MÃ©dio de Espera por Atividade (dias)'); results['chart_14_avg_wait_by_activity'] = fig



Â  Â  handoff_counts = Counter()

Â  Â  for _, trace in log_df.groupby('case:concept:name'):

Â  Â  Â  Â  resources = trace['org:resource'].tolist()

Â  Â  Â  Â  for i in range(len(resources) - 1):

Â  Â  Â  Â  Â  Â  if resources[i] != resources[i+1]: handoff_counts[(resources[i], resources[i+1])] += 1

Â  Â  fig, ax = plt.subplots(figsize=(12, 12)); G = nx.DiGraph();

Â  Â  for (source, target), weight in handoff_counts.items(): G.add_edge(source, target, weight=weight)

Â  Â  pos = nx.spring_layout(G, k=1.2, iterations=50, seed=42)

Â  Â  weights = [G[u][v]['weight'] for u,v in G.edges()]

Â  Â  nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=2500, edge_color='gray', width=[w*0.5 for w in weights], ax=ax, font_size=9, connectionstyle='arc3,rad=0.1')

Â  Â  nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, 'weight'), ax=ax); ax.set_title("Rede Social de ColaboraÃ§Ã£o (Handovers)"); results['social_network'] = fig

Â  Â Â 

Â  Â  df_full_context = dfs['full_context']

Â  Â  resource_role_counts = df_full_context.groupby(['resource_name', 'resource_type']).size().reset_index(name='count')

Â  Â  G_bipartite = nx.Graph(); resources_nodes = resource_role_counts['resource_name'].unique(); roles_nodes = resource_role_counts['resource_type'].unique()

Â  Â  G_bipartite.add_nodes_from(resources_nodes, bipartite=0); G_bipartite.add_nodes_from(roles_nodes, bipartite=1)

Â  Â  for _, row in resource_role_counts.iterrows(): G_bipartite.add_edge(row['resource_name'], row['resource_type'], weight=row['count'])

Â  Â  pos = nx.bipartite_layout(G_bipartite, resources_nodes, align='vertical')

Â  Â  fig, ax = plt.subplots(figsize=(12, 10)); nx.draw_networkx_nodes(G_bipartite, pos, nodelist=resources_nodes, node_color='skyblue', node_size=2000, ax=ax); nx.draw_networkx_nodes(G_bipartite, pos, nodelist=roles_nodes, node_color='lightgreen', node_size=4000, ax=ax); nx.draw_networkx_edges(G_bipartite, pos, width=[d['weight']*0.1 for u,v,d in G_bipartite.edges(data=True)], edge_color='gray', ax=ax); nx.draw_networkx_labels(G_bipartite, pos, font_size=9); nx.draw_networkx_edge_labels(G_bipartite, pos, edge_labels={(u,v):d['weight'] for u,v,d in G_bipartite.edges(data=True)}); ax.set_title('Rede de Recursos por FunÃ§Ã£o'); results['bipartite_network'] = fig



Â  Â  return results



def run_full_analysis():

Â  Â  with st.spinner('A processar os dados e a gerar as 46 anÃ¡lises... Por favor, aguarde.'):

Â  Â  Â  Â  st.session_state.dataframes = load_and_preprocess_data(st.session_state.uploaded_files)

Â  Â  Â  Â  if st.session_state.dataframes:

Â  Â  Â  Â  Â  Â  st.session_state.pre_mining_results = generate_pre_mining_visuals(st.session_state.dataframes)

Â  Â  Â  Â  Â  Â  st.session_state.post_mining_results = generate_post_mining_visuals(st.session_state.dataframes)

Â  Â  Â  Â  Â  Â  st.session_state.analysis_complete = True

Â  Â  Â  Â  Â  Â  st.success('AnÃ¡lise concluÃ­da com sucesso!')

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.error("A anÃ¡lise falhou. Verifique os ficheiros e tente novamente.")



def generate_pdf_report(pre_res, post_res):

Â  Â  pdf = FPDF()

Â  Â  pdf.set_auto_page_break(auto=True, margin=15)

Â  Â Â 

Â  Â  all_results = {**pre_res, **post_res}

Â  Â Â 

Â  Â  pdf.add_page()

Â  Â  pdf.set_font("Arial", 'B', 16)

Â  Â  pdf.cell(0, 10, 'RelatÃ³rio de AnÃ¡lise de Processos', 0, 1, 'C')

Â  Â Â 

Â  Â  with tempfile.TemporaryDirectory() as temp_dir:

Â  Â  Â  Â  for name, fig in all_results.items():

Â  Â  Â  Â  Â  Â  if isinstance(fig, plt.Figure):

Â  Â  Â  Â  Â  Â  Â  Â  title = name.replace('_', ' ').replace('plot', '').replace('chart', '').strip().title()

Â  Â  Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  path = os.path.join(temp_dir, f"{name}.png")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig.savefig(path, format="png", bbox_inches='tight', dpi=150)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if pdf.get_y() > 180:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf.add_page()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf.set_font("Arial", 'B', 12)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf.cell(0, 10, title, 0, 1, 'L')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pdf.image(path, x=10, w=190)

Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Error saving {name}: {e}")

Â  Â Â 

Â  Â  return pdf.output(dest='S').encode('latin-1')

Â  Â Â 

# --- 5. LAYOUT DA APLICAÃ‡ÃƒO (UI) ---

st.title("Dashboard Completo de AnÃ¡lise de Processos")

st.sidebar.title("Painel de Controlo")

menu_selection = st.sidebar.radio(

Â  Â  "Menu", ["1. Carregar Dados", "2. Executar AnÃ¡lise", "3. Visualizar Resultados"],

Â  Â  captions=["FaÃ§a o upload dos 5 ficheiros CSV", "Inicie o processamento dos dados", "Explore o dashboard completo"]

)



if menu_selection == "1. Carregar Dados":

Â  Â  st.header("1. Upload dos Ficheiros CSV")

Â  Â  for name in default_files:

Â  Â  Â  Â  with st.container():

Â  Â  Â  Â  Â  Â  uploaded_file = st.file_uploader(f"Carregar `{name}.csv`", type="csv", key=f"upload_{name}")

Â  Â  Â  Â  Â  Â  if uploaded_file:

Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.uploaded_files[name] = uploaded_file

Â  Â  Â  Â  Â  Â  Â  Â  df_preview = pd.read_csv(uploaded_file); uploaded_file.seek(0)

Â  Â  Â  Â  Â  Â  Â  Â  with st.expander(f"PrÃ©-visualizaÃ§Ã£o de `{name}.csv`", expanded=False):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_preview.head(), height=210)



elif menu_selection == "2. Executar AnÃ¡lise":

Â  Â  st.header("2. ExecuÃ§Ã£o da AnÃ¡lise")

Â  Â  if all(st.session_state.uploaded_files.values()):

Â  Â  Â  Â  if st.button("ğŸš€ Iniciar AnÃ¡lise Completa", type="primary", use_container_width=True):

Â  Â  Â  Â  Â  Â  run_full_analysis()

Â  Â  else:

Â  Â  Â  Â  st.error("Por favor, carregue todos os 5 ficheiros na secÃ§Ã£o '1. Carregar Dados'.")



elif menu_selection == "3. Visualizar Resultados":

Â  Â  st.header("3. Dashboard de Resultados")

Â  Â  if not st.session_state.analysis_complete:

Â  Â  Â  Â  st.warning("A anÃ¡lise ainda nÃ£o foi executada.")

Â  Â  else:

Â  Â  Â  Â  pre_res = st.session_state.pre_mining_results

Â  Â  Â  Â  post_res = st.session_state.post_mining_results

Â  Â  Â  Â Â 

Â  Â  Â  Â  pdf_buffer = generate_pdf_report(pre_res, post_res)

Â  Â  Â  Â  st.sidebar.download_button(

Â  Â  Â  Â  Â  Â  label="ğŸ“¥ Gerar RelatÃ³rio PDF",

Â  Â  Â  Â  Â  Â  data=pdf_buffer,

Â  Â  Â  Â  Â  Â  file_name="relatorio_analise_processos.pdf",

Â  Â  Â  Â  Â  Â  mime="application/pdf",

Â  Â  Â  Â  )



Â  Â  Â  Â  st.sidebar.markdown("---")

Â  Â  Â  Â  st.sidebar.subheader("NavegaÃ§Ã£o do Dashboard")

Â  Â  Â  Â Â 

Â  Â  Â  Â  main_tab = st.sidebar.radio("Ãrea de AnÃ¡lise", ["AnÃ¡lise Descritiva (PrÃ©-MineraÃ§Ã£o)", "AnÃ¡lise de Processos (PÃ³s-MineraÃ§Ã£o)"], label_visibility="collapsed")



Â  Â  Â  Â  if main_tab == "AnÃ¡lise Descritiva (PrÃ©-MineraÃ§Ã£o)":

Â  Â  Â  Â  Â  Â  st.subheader("ğŸ“Š AnÃ¡lise Descritiva (PrÃ©-MineraÃ§Ã£o)")

Â  Â  Â  Â  Â  Â  sections = ["VisÃ£o Geral e KPIs", "Performance e Prazos", "Organizacional e Custos", "Gargalos e Handoffs"]

Â  Â  Â  Â  Â  Â  selected_section = st.sidebar.selectbox("SecÃ§Ã£o:", sections)



Â  Â  Â  Â  Â  Â  if selected_section == "VisÃ£o Geral e KPIs":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("VisÃ£o Geral e KPIs")

Â  Â  Â  Â  Â  Â  Â  Â  cols = st.columns(len(pre_res.get('kpis', {})))

Â  Â  Â  Â  Â  Â  Â  Â  for i, (metric, value) in enumerate(pre_res.get('kpis', {}).items()): cols[i].metric(label=metric, value=str(value))

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_01' in pre_res: st.pyplot(pre_res['plot_01'])



Â  Â  Â  Â  Â  Â  if selected_section == "Performance e Prazos":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise de Performance e Prazos")

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_02' in pre_res: st.pyplot(pre_res['plot_02'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_03' in pre_res: st.pyplot(pre_res['plot_03'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_04_05' in pre_res: st.pyplot(pre_res['plot_04_05'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_06' in pre_res: st.pyplot(pre_res['plot_06'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_17' in pre_res: st.pyplot(pre_res['plot_17'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_18' in pre_res: st.pyplot(pre_res['plot_18'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_25' in pre_res: st.pyplot(pre_res['plot_25'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_26' in pre_res: st.pyplot(pre_res['plot_26'])



Â  Â  Â  Â  Â  Â  if selected_section == "Organizacional e Custos":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise Organizacional, Atividades e Custos")

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_07' in pre_res: st.pyplot(pre_res['plot_07'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_10' in pre_res: st.pyplot(pre_res['plot_10'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_11' in pre_res: st.pyplot(pre_res['plot_11'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_12' in pre_res: st.pyplot(pre_res['plot_12'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_15' in pre_res: st.pyplot(pre_res['plot_15'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_19' in pre_res: st.pyplot(pre_res['plot_19'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_13' in pre_res: st.pyplot(pre_res['plot_13'])



Â  Â  Â  Â  Â  Â  if selected_section == "Gargalos e Handoffs":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise de Gargalos e Handoffs")

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_08' in pre_res: st.pyplot(pre_res['plot_08'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_09' in pre_res: st.pyplot(pre_res['plot_09'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_14' in pre_res: st.pyplot(pre_res['plot_14'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_20' in pre_res: st.pyplot(pre_res['plot_20'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_21' in pre_res: st.pyplot(pre_res['plot_21'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_22' in pre_res: st.pyplot(pre_res['plot_22'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_23' in pre_res: st.pyplot(pre_res['plot_23'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'plot_24' in pre_res: st.pyplot(pre_res['plot_24'])

Â  Â  Â  Â Â 

Â  Â  Â  Â  if main_tab == "AnÃ¡lise de Processos (PÃ³s-MineraÃ§Ã£o)":

Â  Â  Â  Â  Â  Â  st.subheader("ğŸ—ºï¸ AnÃ¡lise de Processos (PÃ³s-MineraÃ§Ã£o)")

Â  Â  Â  Â  Â  Â  sections = ["Descoberta de Modelos", "Variantes e Conformidade", "AnÃ¡lise Temporal", "Tempos de Espera e Recursos"]

Â  Â  Â  Â  Â  Â  selected_section = st.sidebar.selectbox("SecÃ§Ã£o:", sections, key='tab2_selectbox')



Â  Â  Â  Â  Â  Â  if selected_section == "Descoberta de Modelos":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Descoberta de Modelos e MÃ©tricas de Qualidade")

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("##### Modelo com Inductive Miner")

Â  Â  Â  Â  Â  Â  Â  Â  if 'model_01_inductive' in post_res: st.graphviz_chart(post_res['model_01_inductive'])

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("###### MÃ©tricas de Qualidade (Inductive)")

Â  Â  Â  Â  Â  Â  Â  Â  if 'metrics_inductive' in post_res: st.pyplot(post_res['metrics_inductive'])

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("##### Modelo com Heuristics Miner")

Â  Â  Â  Â  Â  Â  Â  Â  if 'model_02_heuristics' in post_res: st.graphviz_chart(post_res['model_02_heuristics'])

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("###### MÃ©tricas de Qualidade (Heuristics)")

Â  Â  Â  Â  Â  Â  Â  Â  if 'metrics_heuristics' in post_res: st.pyplot(post_res['metrics_heuristics'])

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Mapa de Performance do Processo")

Â  Â  Â  Â  Â  Â  Â  Â  if 'model_03_performance_dfg' in post_res: st.graphviz_chart(post_res['model_03_performance_dfg'])



Â  Â  Â  Â  Â  Â  if selected_section == "Variantes e Conformidade":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise de Variantes e Conformidade")

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_04_variants_pie' in post_res: st.pyplot(post_res['chart_04_variants_pie'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_08_variant_duration' in post_res: st.pyplot(post_res['chart_08_variant_duration'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_05_conformance_fitness' in post_res: st.pyplot(post_res['chart_05_conformance_fitness'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_09_deviation_scatter' in post_res: st.pyplot(post_res['chart_09_deviation_scatter'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_10_conformance_over_time' in post_res: st.pyplot(post_res['chart_10_conformance_over_time'])



Â  Â  Â  Â  Â  Â  if selected_section == "AnÃ¡lise Temporal":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise Temporal e de Linha do Tempo")

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_06_kpi_time_series' in post_res: st.pyplot(post_res['chart_06_kpi_time_series'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_11_cumulative_throughput' in post_res: st.pyplot(post_res['chart_11_cumulative_throughput'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_07_gantt_chart' in post_res: st.pyplot(post_res['chart_07_gantt_chart'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_12_milestone_analysis' in post_res: st.pyplot(post_res['chart_12_milestone_analysis'])



Â  Â  Â  Â  Â  Â  if selected_section == "Tempos de Espera e Recursos":

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise de Tempos de Espera e Recursos")

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_13_waiting_time_matrix' in post_res: st.pyplot(post_res['chart_13_waiting_time_matrix'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'chart_14_avg_wait_by_activity' in post_res: st.pyplot(post_res['chart_14_avg_wait_by_activity'])

Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Redes de ColaboraÃ§Ã£o entre Recursos")

Â  Â  Â  Â  Â  Â  Â  Â  if 'social_network' in post_res: st.pyplot(post_res['social_network'])

Â  Â  Â  Â  Â  Â  Â  Â  if 'bipartite_network' in post_res: st.pyplot(post_res['bipartite_network'])
